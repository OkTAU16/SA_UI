{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import scipy.io\n",
    "import Rbeast as rb\n",
    "from SLM_tools import *\n",
    "from scipy.io import savemat\n",
    "import pickle\n",
    "import re\n",
    "import time"
   ],
   "id": "1dfc04cc15ce7fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "top_dir = r\"C:\\Users\\User\\OneDrive - mail.tau.ac.il\\Documents\\Final Project\\Project2-Omri and Idan\\Results\" \n",
    "\n",
    "pattern = re.compile(\n",
    "    r\"(?P<type>distance|energy)_vec_mu_(?P<mu>[-+]?\\d*\\.\\d+|\\d+)_energy_(?P<energy>[-+]?\\d*\\.\\d+|\\d+)_run_num_(?P<run_num>\\d+)_total_num_target_\\d+\\.mat\"\n",
    ")\n",
    "distance_paths = []\n",
    "energy_paths = []\n",
    "seen_distance_paths = set()\n",
    "seen_energy_paths = set()\n",
    "mu = 1.6\n",
    "path = r\"C:\\Users\\User\\OneDrive - mail.tau.ac.il\\Documents\\SA_UI\\testing data\"\n",
    "# save_path = os.path.join(path, rf\"{mu}_v3\")\n",
    "# os.mkdir(save_path)\n",
    "save_path = r\"C:\\Users\\User\\OneDrive - mail.tau.ac.il\\Documents\\SA_UI\\testing data\\merged_data_1_6\"\n",
    "for root, dirs, files in os.walk(top_dir):\n",
    "    for file in files:\n",
    "        match = pattern.match(file)\n",
    "        if match:\n",
    "            file_info = match.groupdict()\n",
    "            if file_info['mu'] == str(mu):\n",
    "                full_path = os.path.join(root, file)\n",
    "                if file_info['type'] == 'distance':\n",
    "                    if full_path not in seen_distance_paths:\n",
    "                        distance_paths.append(full_path)\n",
    "                        seen_distance_paths.add(full_path)\n",
    "                elif file_info['type'] == 'energy':\n",
    "                    if full_path not in seen_energy_paths:\n",
    "                        energy_paths.append(full_path)\n",
    "                        seen_energy_paths.add(full_path)\n"
   ],
   "id": "1e8eb91d074f320",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f2f4a3ce46ce9532",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "C = []\n",
    "downsampling_factor = 10000\n",
    "n_components = 3\n",
    "features_extracted = 0\n",
    "total_runtime = 0\n",
    "files_with_assembly = 0\n",
    "total_data_files = len(energy_paths)\n",
    "log = []\n",
    "counters = {\"load\":[],\"extraction\":[],\"segmentation\":[],\"pca\":None,\"extracted_features\":[]}\n",
    "for i, (energy_path, distance_path) in enumerate(zip(energy_paths, distance_paths)):\n",
    "    start1 = time.time()\n",
    "    distance_data = scipy.io.loadmat(distance_path)['foo']\n",
    "    energy_data = scipy.io.loadmat(energy_path)['foo'].T\n",
    "    energy_data, distance_data, time_vec = SLM_tools.downsample(energy_data, distance_data,downsampling_factor)\n",
    "    merged = np.concatenate((energy_data, distance_data), axis=1)\n",
    "    merged_filename = os.path.join(save_path, f\"merged_energy_distance_{i}_mu_{mu}.mat\")\n",
    "    sio.savemat(merged_filename, {\"energy_distance\":merged})"
   ],
   "id": "ca5845b0b992cd81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    loading_time = round(time.time() - start1,3)\n",
    "    log1 = f\"\\nfile {i+1}/{len(energy_paths)}  loaded: {os.path.relpath(energy_path,top_dir)} Loading time: {loading_time} seconds\"\n",
    "    print(log1)\n",
    "    log.append(log1)\n",
    "    counters[\"load\"].append(loading_time)\n",
    "    start2 = time.time()\n",
    "    o, cp, mean_trend = SLM_tools.beast(energy_data)\n",
    "    segmentation_time = round(time.time() - start2,3)\n",
    "    log2 = f\"\\nfinished segmentation(Beast), runtime: {segmentation_time} seconds\"\n",
    "    print(log2)\n",
    "    log.append(log2)\n",
    "    counters[\"segmentation\"].append(segmentation_time)\n",
    "    start3 = time.time()\n",
    "    A = SLM_tools.feature_extraction(energy_data, distance_data,mean_trend,cp)\n",
    "    extraction_time = round(time.time() - start3,3)\n",
    "    print(f\"\\nfinished feature extraction, runtime: {extraction_time} seconds\")\n",
    "    counters[\"extraction\"].append(extraction_time)\n",
    "    print(f\"\\nAssembly? : {len(A) > 0}\")\n",
    "    if len(A) > 0:\n",
    "        files_with_assembly += 1\n",
    "        features_extracted += A[0].shape[0]\n",
    "        counters[\"extracted_features\"].append(A[0].shape[0])\n",
    "        log3 = f\"\\nfiles_with_assembly: {files_with_assembly}/{total_data_files}\"\n",
    "        print(log3)\n",
    "        log.append(log3)\n",
    "        log4 = f\"\\nTotal features_extracted: {features_extracted}\"\n",
    "        print(log4)\n",
    "        log.append(log4)\n",
    "    else:\n",
    "        log5 = f\"\\nNo Assembly, No features extracted\"\n",
    "        print(log5)\n",
    "        log.append(log5)\n",
    "    C.append(A)\n",
    "    total_runtime += (loading_time + extraction_time + segmentation_time)\n",
    "log6 = f\"\\nFinished pre-pca processing,total runtime {total_runtime} for {len(energy_paths)} files\"\n",
    "print(log6)\n",
    "log.append(log6)\n",
    "c_path = os.path.join(save_path,f\"C_all_mu_{mu}.pkl\")\n",
    "with open(c_path, \"wb\") as f:\n",
    "    pickle.dump(C, f)\n",
    "log7 = \"\\nsaved all processed data\"\n",
    "print(log7)\n",
    "log.append(log7)\n",
    "start_pca = time.time()\n",
    "log8 = f\"\\nStarting PCA\"\n",
    "print(log8)\n",
    "log.append(log8)\n",
    "c_reduced = SLM_tools.feature_selection(C)\n",
    "principal_components, score, latent = SLM_tools.pca(c_reduced,n_components)\n",
    "a_reduced = SLM_tools.data_preparation(score, c_reduced,n_components)\n",
    "counters[\"selected_features\"] = a_reduced.shape[0]\n",
    "runtime_pca = round(time.time() - start_pca,3)\n",
    "counters[\"pca\"] = runtime_pca\n",
    "total_runtime += runtime_pca\n",
    "log9 = f\"\\nFinished PCA runtime {runtime_pca} seconds\"\n",
    "print(log9)\n",
    "log.append(log9)\n",
    "a_path = os.path.join(save_path,f\"a_reduced_all_mu_{mu}.mat\")\n",
    "savemat(a_path, {'a_reduced': a_reduced})\n",
    "log10 = f\"\\nsaved all extracted features , total extracted features {features_extracted} total runtime {total_runtime} seconds\"\n",
    "log.append(log10)\n",
    "log.append(\"SUMMARY\")\n",
    "log11 = f\"\\nfiles {total_data_files} \\nfiles_with_assembly: {files_with_assembly}/{total_data_files} \\nmean extracted features {np.mean(counters['extracted_features'])} \\ntotal extracted features {features_extracted} \\ntotal_selected_features {counters['selected_features']} \\ntotal runtime {total_runtime} seconds \\nmean loading time {np.mean(counters['load'])} \\nmean segmentation time {np.mean(counters['segmentation'])} \\nmean extraction time {np.mean(counters['extraction'])} \\nPCA time {counters['pca']}\"\n",
    "log.append(log11)\n",
    "print(log11)\n",
    "log_path = os.path.join(save_path,f\"run_log_mu_{mu}.txt\",)\n",
    "with open(log_path,'w') as f:\n",
    "    for line in log:\n",
    "        f.write(line)\n",
    "        "
   ],
   "id": "60910e7d27e03f29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T19:51:49.664861Z",
     "start_time": "2024-08-21T19:51:43.842558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import scipy.io\n",
    "import Rbeast as rb\n",
    "from SLM_tools import *\n",
    "from scipy.io import savemat\n",
    "import pickle\n",
    "import re\n",
    "import time"
   ],
   "id": "ad2cad572e7027f3",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T19:51:49.680780Z",
     "start_time": "2024-08-21T19:51:49.667542Z"
    }
   },
   "cell_type": "code",
   "source": "a_reduced = sio.loadmat(r\"C:\\Users\\User\\OneDrive - mail.tau.ac.il\\Documents\\SA_UI_TEMP\\testing data\\out\\out_21_8\\selected_features_21_08_20_41.mat\")[\"selected_features\"]",
   "id": "d4089c578d3516c3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T19:51:49.696406Z",
     "start_time": "2024-08-21T19:51:49.680780Z"
    }
   },
   "cell_type": "code",
   "source": "a_reduced.shape",
   "id": "ac3004cdc906a708",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2615, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T19:51:49.712029Z",
     "start_time": "2024-08-21T19:51:49.696406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = r\"C:\\Users\\User\\OneDrive - mail.tau.ac.il\\Documents\\SA_UI\\testing data\"\n",
    "save_path = r\"C:\\Users\\User\\OneDrive - mail.tau.ac.il\\Documents\\SA_UI_TEMP\\testing data\\out\\out_21_8\\run_2\"\n",
    "os.makedirs(save_path,exist_ok=True)"
   ],
   "id": "1b13874ae4827107",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T19:52:32.380644Z",
     "start_time": "2024-08-21T19:51:49.712029Z"
    }
   },
   "cell_type": "code",
   "source": "YI, tfas_predict_mat, tfas_actually_mat, mean_error_mat,random_x = SLM_tools.model_training_with_cv(a_reduced,n_components=3,cv_num=10)",
   "id": "cca3f3fdb52ac460",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T19:52:32.602175Z",
     "start_time": "2024-08-21T19:52:32.382485Z"
    }
   },
   "cell_type": "code",
   "source": "SLM_tools.draw_stochastic_landscape_2d(a_reduced,save_path,3)",
   "id": "e49812d6d3304648",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T19:52:32.909775Z",
     "start_time": "2024-08-21T19:52:32.602175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cv_num = 10\n",
    "mean_vec, std_vec, hist_space, x_hist_space, x_ticks, y_ticks = SLM_tools.model_eval(tfas_predict_mat, tfas_actually_mat, cv_num, save_path)"
   ],
   "id": "1f573e1bcbaf88e4",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T19:52:37.174749Z",
     "start_time": "2024-08-21T19:52:32.911427Z"
    }
   },
   "cell_type": "code",
   "source": "tfas_predict_mat_2, tfas_actually_mat_2, mean_error_mat_2 = SLM_tools.train_again_on_validation(a_reduced, n_components=3)",
   "id": "b0fcdfb79b6312ea",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T19:52:37.797454Z",
     "start_time": "2024-08-21T19:52:37.174749Z"
    }
   },
   "cell_type": "code",
   "source": "SLM_tools.cv_bias_correction(tfas_predict_mat_2=tfas_predict_mat_2, tfas_actually_mat_2=tfas_actually_mat_2, hist_space=hist_space, mean_vec=mean_vec, x_hist_space=x_hist_space,x_ticks=x_ticks,y_ticks=y_ticks,save_path=save_path)",
   "id": "803562acd8b4be2",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = sio.loadmat(r\"C:\\Users\\User\\OneDrive - mail.tau.ac.il\\Documents\\SA_UI\\data_for_box2.mat\")[\"data\"]\n",
    "YY = sio.loadmat(r\"C:\\Users\\User\\OneDrive - mail.tau.ac.il\\Documents\\SA_UI\\YY_for_box2.mat\")[\"YY\"]\n",
    "fig, ax = plt.subplots()\n",
    "flat_data = []\n",
    "for sublist_group in data:\n",
    "    for arr in sublist_group:\n",
    "        if arr.ndim > 2:\n",
    "            raise ValueError(f\"Array with shape {arr.shape} is too high-dimensional for a boxplot.\")\n",
    "        elif arr.ndim == 2:\n",
    "            # Flatten the array to 1D, or choose a specific axis, depending on the data\n",
    "            arr = arr.flatten()\n",
    "        flat_data.append(arr)\n",
    "bp = ax.boxplot(flat_data, positions=np.round(YY.flatten(), 2), patch_artist=True)\n",
    "for i, box in enumerate(bp['boxes']):\n",
    "    box.set(facecolor='white', edgecolor='blue', linewidth=2)\n",
    "\n",
    "# Set color for medians, whiskers, and caps\n",
    "for i, element in enumerate(bp['medians'] + bp['whiskers'] + bp['caps']):\n",
    "    element.set_color('black')\n",
    "\n",
    "# Set color for fliers\n",
    "for i, element in enumerate(bp['fliers']):\n",
    "    element.set_markeredgecolor('black')\n",
    "plt.show()"
   ],
   "id": "56f16e2a8a0a2fc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%reset -f",
   "id": "33c2da8da1fa6033",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import scipy.io\n",
    "import Rbeast as rb\n",
    "from SLM_tools import *\n",
    "from scipy.io import savemat\n",
    "import pickle\n",
    "import re\n",
    "import time"
   ],
   "id": "b288aa08260ce060",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "top_dir = r\"C:\\Users\\User\\OneDrive - mail.tau.ac.il\\Documents\\SA_UI_TEMP\\testing data\\irregular_data\"\n",
    "\n",
    "pattern = re.compile(\n",
    "    r\"mini_(?P<type>denial|summed_distance_UP|summed_energy_UP)_vec_mu_(?P<mu>[-+]?\\d*\\.\\d+|\\d+)_energy_(?P<energy>[-+]?\\d*\\.\\d+|\\d+)_run_num_(?P<run_num>\\d+)_total_num_target_\\d+\\.mat\"\n",
    ")\n",
    "\n",
    "denial_paths = []\n",
    "summed_distance_UP_paths = []\n",
    "summed_energy_UP_paths = []\n",
    "seen_denial_paths = set()\n",
    "seen_summed_distance_UP_paths = set()\n",
    "seen_summed_energy_UP_paths = set()\n",
    "\n",
    "mu = 3  # Set this to the desired mu value\n",
    "energy = 3.5  # Set this to the desired energy value\n",
    "\n",
    "save_path = r\"C:\\Users\\User\\OneDrive - mail.tau.ac.il\\Documents\\SA_UI_TEMP\\testing data\\meged_data_deniel_energy_3_5_distance_m_3\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "for root, dirs, files in os.walk(top_dir):\n",
    "    for file in files:\n",
    "        match = pattern.match(file)\n",
    "        if match:\n",
    "            file_info = match.groupdict()\n",
    "            if file_info['energy'] == str(energy) and file_info['mu'] == str(mu) :\n",
    "                full_path = os.path.join(root, file)\n",
    "                if file_info['type'] == 'denial':\n",
    "                    if full_path not in seen_denial_paths:\n",
    "                        denial_paths.append(full_path)\n",
    "                        seen_denial_paths.add(full_path)\n",
    "                elif file_info['type'] == 'summed_distance_UP':\n",
    "                    if full_path not in seen_summed_distance_UP_paths:\n",
    "                        summed_distance_UP_paths.append(full_path)\n",
    "                        seen_summed_distance_UP_paths.add(full_path)\n",
    "                elif file_info['type'] == 'summed_energy_UP':\n",
    "                    if full_path not in seen_summed_energy_UP_paths:\n",
    "                        summed_energy_UP_paths.append(full_path)\n",
    "                        seen_summed_energy_UP_paths.add(full_path)"
   ],
   "id": "a75765d726552eb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"energy_len: {len(summed_energy_UP_paths)},distance_len: {len(summed_distance_UP_paths)}, denial_len: {len(denial_paths)}\")",
   "id": "af9a61794034607",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "zeros = 0\n",
    "for i in range(len(denial_paths)):\n",
    "    denial_data = np.cumsum(scipy.io.loadmat(denial_paths[i])['foo'][:,0])\n",
    "    summed_energy_UP_data = np.squeeze(scipy.io.loadmat(summed_energy_UP_paths[i])['foo'])\n",
    "    summed_distance_UP_data = scipy.io.loadmat(summed_distance_UP_paths[i])['foo']\n",
    "    # values_vec, distance_data, time_vec = SLM_tools.interpolate_data_over_regular_time(summed_energy_UP_data, summed_distance_UP_data, denial_data)\n",
    "    zeros +=np.count_nonzero(summed_distance_UP_data == 0)"
   ],
   "id": "127305cdcec5c486",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "zeros",
   "id": "a3cc8f793618bff1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "zeros",
   "id": "14cef3b73bd10aa5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1495e29ba8cca8a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.count_nonzero(distance_data == 0)",
   "id": "618252041ee8d0d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# downsampling_factor = 1  # Set this to your desired downsampling factor\n",
    "for i, (denial_path, summed_energy_UP_path, summed_distance_UP_path) in enumerate(zip(denial_paths, summed_energy_UP_paths, summed_distance_UP_paths)):\n",
    "    start1 = time.time()\n",
    "    \n",
    "    denial_data = np.cumsum(scipy.io.loadmat(denial_path)['foo'][:,0])\n",
    "    summed_energy_UP_data = np.squeeze(scipy.io.loadmat(summed_energy_UP_path)['foo'])\n",
    "    summed_distance_UP_data = scipy.io.loadmat(summed_distance_UP_path)['foo']\n",
    "    \n",
    "    # Assuming we need to downsample all three datasets\n",
    "    # denial_data, summed_energy_UP_data, time_vec = SLM_tools.downsample(denial_data, summed_energy_UP_data, downsampling_factor)\n",
    "    # _, summed_distance_UP_data, _ = SLM_tools.downsample(denial_data, summed_distance_UP_data, downsampling_factor)\n",
    "    \n",
    "    merged = np.concatenate((np.reshape(denial_data,(len(denial_data),1)), np.reshape(summed_energy_UP_data,(len(summed_energy_UP_data),1)), summed_distance_UP_data), axis=1)\n",
    "    \n",
    "    merged_filename = os.path.join(save_path, f\"merged_denial_energy_distance_{i}_mu_{mu}_energy_{2.2}.mat\")\n",
    "    sio.savemat(merged_filename, {\"merged_data\": merged})\n",
    "    \n",
    "    end1 = time.time()\n",
    "    print(f\"Processed and saved file {i+1}/{len(denial_paths)} in {end1 - start1:.2f} seconds\")\n",
    "\n",
    "print(\"All files processed and saved.\")"
   ],
   "id": "56f56be0a6587374",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from SLM_tools import *\n",
    "dir_path = r\"C:\\Users\\User\\OneDrive - mail.tau.ac.il\\Documents\\SA_UI_TEMP\\testing data\\merged_data_1_6\"\n",
    "n_components = 3\n",
    "downsampling_factor = None\n",
    "target_num = 2\n",
    "time_vec_exists = False\n",
    "cv_num = 10 \n",
    "save_path = r\"C:\\Users\\User\\OneDrive - mail.tau.ac.il\\Documents\\SA_UI_TEMP\\testing data\\out\\out_2\"\n",
    "os.makedirs(save_path,exist_ok=True)\n",
    "data_variable_name = \"energy_distance\"\n",
    "SLM_tools.create_and_evaluate_stochastic_landscape(dir_path,n_components, downsampling_factor, target_num, time_vec_exists, cv_num, save_path, data_variable_name)"
   ],
   "id": "a17897054c2dea7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T11:53:41.305814Z",
     "start_time": "2024-08-28T11:53:40.615602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "img = cv2.imread(\"SLM_logo.png\")\n",
    "img = cv2.resize(img,(500,1024))\n",
    "cv2.imshow(\"img\",img)\n",
    "cv2.waitKey(0)"
   ],
   "id": "20035df0815d54a2",
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) C:\\b\\abs_f8n1j3l9l0\\croot\\opencv-suite_1691622637237\\work\\modules\\highgui\\src\\window.cpp:1267: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31merror\u001B[0m                                     Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m img \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mimread(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSLM_logo.png\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m img \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mresize(img,(\u001B[38;5;241m500\u001B[39m,\u001B[38;5;241m1024\u001B[39m))\n\u001B[1;32m----> 4\u001B[0m cv2\u001B[38;5;241m.\u001B[39mimshow(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimg\u001B[39m\u001B[38;5;124m\"\u001B[39m,img)\n\u001B[0;32m      5\u001B[0m cv2\u001B[38;5;241m.\u001B[39mwaitKey(\u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[1;31merror\u001B[0m: OpenCV(4.6.0) C:\\b\\abs_f8n1j3l9l0\\croot\\opencv-suite_1691622637237\\work\\modules\\highgui\\src\\window.cpp:1267: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-08-29T09:07:46.816017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from SLM_tools import *\n",
    "import scipy as sio\n",
    "feature_matrix = sio.loadmat(r\"C:\\Users\\Omri.k\\Desktop\\temp\\testing data\\testing data\\out\\out_28_8\\29_8\\selected_features_29_08_11_35.mat\")[\"feature_matrix\"]\n",
    "save_path = r\"C:\\Users\\Omri.k\\Desktop\\temp\\testing data\\testing data\\out\\out_28_8\\29_8\\tests\""
   ],
   "id": "28c06f167216c999",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "feature_matrix.shape",
   "id": "1ea4a5cadca590d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "YI, tfas_predict_mat, tfas_actually_mat, mean_error_mat, random_x = SLM_tools.model_training_with_cv(feature_matrix,\n",
    "                                                                                                     3,\n",
    "                                                                                                    int(10))\n"
   ],
   "id": "a42c72a9b1507194"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mean_vec, std_vec, hist_space, x_hist_space, x_ticks, y_ticks = SLM_tools.model_eval(tfas_predict_mat,\n",
    "                                                                                             tfas_actually_mat,\n",
    "                                                                                             int(10),\n",
    "                                                                                             save_path)"
   ],
   "id": "28dfebf84290cd0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tfas_predict_mat_2, tfas_actually_mat_2, mean_error_mat_2 = SLM_tools.train_again_on_validation(feature_matrix,\n",
    "                                                                                                        3)\n",
    "SLM_tools.cv_bias_correction(tfas_predict_mat_2, tfas_actually_mat_2, hist_space, mean_vec, x_hist_space,\n",
    "                                     x_ticks, y_ticks, save_path)"
   ],
   "id": "784719e81b8fe680"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
