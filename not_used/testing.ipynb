{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%reset -f",
   "id": "33c2da8da1fa6033",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import scipy.io\n",
    "import Rbeast as rb\n",
    "from SLM_tools import *\n",
    "from scipy.io import savemat\n",
    "import pickle\n",
    "import re\n",
    "import time"
   ],
   "id": "b288aa08260ce060",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "top_dir = r\"C:\\Users\\User\\OneDrive - mail.tau.ac.il\\Documents\\Final Project\\Project2-Omri and Idan\\Results\" \n",
    "\n",
    "pattern = re.compile(\n",
    "    r\"(?P<type>distance|energy)_vec_mu_(?P<mu>[-+]?\\d*\\.\\d+|\\d+)_energy_(?P<energy>[-+]?\\d*\\.\\d+|\\d+)_run_num_(?P<run_num>\\d+)_total_num_target_\\d+\\.mat\"\n",
    ")\n",
    "distance_paths = []\n",
    "energy_paths = []\n",
    "seen_distance_paths = set()\n",
    "seen_energy_paths = set()\n",
    "mu = 1.6\n",
    "path = r\"C:\\Users\\User\\OneDrive - mail.tau.ac.il\\Documents\\SA_UI\\testing data\"\n",
    "# save_path = os.path.join(path, rf\"{mu}_v3\")\n",
    "# os.mkdir(save_path)\n",
    "save_path = r\"C:\\Users\\User\\OneDrive - mail.tau.ac.il\\Documents\\SA_UI\\testing data\\merged_data_1_6\"\n",
    "for root, dirs, files in os.walk(top_dir):\n",
    "    for file in files:\n",
    "        match = pattern.match(file)\n",
    "        if match:\n",
    "            file_info = match.groupdict()\n",
    "            if file_info['mu'] == str(mu):\n",
    "                full_path = os.path.join(root, file)\n",
    "                if file_info['type'] == 'distance':\n",
    "                    if full_path not in seen_distance_paths:\n",
    "                        distance_paths.append(full_path)\n",
    "                        seen_distance_paths.add(full_path)\n",
    "                elif file_info['type'] == 'energy':\n",
    "                    if full_path not in seen_energy_paths:\n",
    "                        energy_paths.append(full_path)\n",
    "                        seen_energy_paths.add(full_path)\n"
   ],
   "id": "1e8eb91d074f320",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(energy_paths)",
   "id": "af9a61794034607",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "C = []\n",
    "downsampling_factor = 10000\n",
    "n_components = 3\n",
    "features_extracted = 0\n",
    "total_runtime = 0\n",
    "files_with_assembly = 0\n",
    "total_data_files = len(energy_paths)\n",
    "log = []\n",
    "counters = {\"load\":[],\"extraction\":[],\"segmentation\":[],\"pca\":None,\"extracted_features\":[]}\n",
    "for i, (energy_path, distance_path) in enumerate(zip(energy_paths, distance_paths)):\n",
    "    start1 = time.time()\n",
    "    distance_data = scipy.io.loadmat(distance_path)['foo']\n",
    "    energy_data = scipy.io.loadmat(energy_path)['foo'].T\n",
    "    energy_data, distance_data, time_vec = SLM_tools.downsample(energy_data, distance_data,downsampling_factor)\n",
    "    merged = np.concatenate((energy_data, distance_data), axis=1)\n",
    "    merged_filename = os.path.join(save_path, f\"merged_energy_distance_{i}_mu_{mu}.mat\")\n",
    "    sio.savemat(merged_filename, {\"energy_distance\":merged})"
   ],
   "id": "ca5845b0b992cd81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    loading_time = round(time.time() - start1,3)\n",
    "    log1 = f\"\\nfile {i+1}/{len(energy_paths)}  loaded: {os.path.relpath(energy_path,top_dir)} Loading time: {loading_time} seconds\"\n",
    "    print(log1)\n",
    "    log.append(log1)\n",
    "    counters[\"load\"].append(loading_time)\n",
    "    start2 = time.time()\n",
    "    o, cp, mean_trend = SLM_tools.beast(energy_data)\n",
    "    segmentation_time = round(time.time() - start2,3)\n",
    "    log2 = f\"\\nfinished segmentation(Beast), runtime: {segmentation_time} seconds\"\n",
    "    print(log2)\n",
    "    log.append(log2)\n",
    "    counters[\"segmentation\"].append(segmentation_time)\n",
    "    start3 = time.time()\n",
    "    A = SLM_tools.feature_extraction(energy_data, distance_data,mean_trend,cp)\n",
    "    extraction_time = round(time.time() - start3,3)\n",
    "    print(f\"\\nfinished feature extraction, runtime: {extraction_time} seconds\")\n",
    "    counters[\"extraction\"].append(extraction_time)\n",
    "    print(f\"\\nAssembly? : {len(A) > 0}\")\n",
    "    if len(A) > 0:\n",
    "        files_with_assembly += 1\n",
    "        features_extracted += A[0].shape[0]\n",
    "        counters[\"extracted_features\"].append(A[0].shape[0])\n",
    "        log3 = f\"\\nfiles_with_assembly: {files_with_assembly}/{total_data_files}\"\n",
    "        print(log3)\n",
    "        log.append(log3)\n",
    "        log4 = f\"\\nTotal features_extracted: {features_extracted}\"\n",
    "        print(log4)\n",
    "        log.append(log4)\n",
    "    else:\n",
    "        log5 = f\"\\nNo Assembly, No features extracted\"\n",
    "        print(log5)\n",
    "        log.append(log5)\n",
    "    C.append(A)\n",
    "    total_runtime += (loading_time + extraction_time + segmentation_time)\n",
    "log6 = f\"\\nFinished pre-pca processing,total runtime {total_runtime} for {len(energy_paths)} files\"\n",
    "print(log6)\n",
    "log.append(log6)\n",
    "c_path = os.path.join(save_path,f\"C_all_mu_{mu}.pkl\")\n",
    "with open(c_path, \"wb\") as f:\n",
    "    pickle.dump(C, f)\n",
    "log7 = \"\\nsaved all processed data\"\n",
    "print(log7)\n",
    "log.append(log7)\n",
    "start_pca = time.time()\n",
    "log8 = f\"\\nStarting PCA\"\n",
    "print(log8)\n",
    "log.append(log8)\n",
    "c_reduced = SLM_tools.feature_selection(C)\n",
    "principal_components, score, latent = SLM_tools.pca(c_reduced,n_components)\n",
    "a_reduced = SLM_tools.data_preparation(score, c_reduced,n_components)\n",
    "counters[\"selected_features\"] = a_reduced.shape[0]\n",
    "runtime_pca = round(time.time() - start_pca,3)\n",
    "counters[\"pca\"] = runtime_pca\n",
    "total_runtime += runtime_pca\n",
    "log9 = f\"\\nFinished PCA runtime {runtime_pca} seconds\"\n",
    "print(log9)\n",
    "log.append(log9)\n",
    "a_path = os.path.join(save_path,f\"a_reduced_all_mu_{mu}.mat\")\n",
    "savemat(a_path, {'a_reduced': a_reduced})\n",
    "log10 = f\"\\nsaved all extracted features , total extracted features {features_extracted} total runtime {total_runtime} seconds\"\n",
    "log.append(log10)\n",
    "log.append(\"SUMMARY\")\n",
    "log11 = f\"\\nfiles {total_data_files} \\nfiles_with_assembly: {files_with_assembly}/{total_data_files} \\nmean extracted features {np.mean(counters['extracted_features'])} \\ntotal extracted features {features_extracted} \\ntotal_selected_features {counters['selected_features']} \\ntotal runtime {total_runtime} seconds \\nmean loading time {np.mean(counters['load'])} \\nmean segmentation time {np.mean(counters['segmentation'])} \\nmean extraction time {np.mean(counters['extraction'])} \\nPCA time {counters['pca']}\"\n",
    "log.append(log11)\n",
    "print(log11)\n",
    "log_path = os.path.join(save_path,f\"run_log_mu_{mu}.txt\",)\n",
    "with open(log_path,'w') as f:\n",
    "    for line in log:\n",
    "        f.write(line)\n",
    "        "
   ],
   "id": "60910e7d27e03f29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a_reduced.shape",
   "id": "1e1ec1c258973b00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a_reduced_2 = sio.loadmat(r\"C:\\Users\\User\\OneDrive - mail.tau.ac.il\\Documents\\SA_UI\\selected_features_03_08_17_46.mat\")[\"selected_features\"]",
   "id": "795525a2d0b753ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a_reduced_2.shape",
   "id": "e3d3a98f40386bbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a_reduced = sio.loadmat(r\"C:\\Users\\User\\OneDrive - mail.tau.ac.il\\Documents\\SA_UI\\testing data\\1.6_v3\\a_reduced_all_mu_1.6.mat\")[\"a_reduced\"]",
   "id": "d4089c578d3516c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a_reduced.shape",
   "id": "b79388bbdb49ebf8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import scipy.io\n",
    "import Rbeast as rb\n",
    "from SLM_tools import *\n",
    "from scipy.io import savemat\n",
    "import pickle\n",
    "import re\n",
    "import time"
   ],
   "id": "1f62d8beb5f0ee01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "a_reduced = sio.loadmat(r\"C:\\Users\\User\\OneDrive - mail.tau.ac.il\\Documents\\SA_UI\\testing data\\1.6_v3\\a_reduced_all_mu_1.6.mat\")[\"a_reduced\"]\n",
    "path = r\"C:\\Users\\User\\OneDrive - mail.tau.ac.il\\Documents\\SA_UI\\testing data\"\n",
    "save_path = os.path.join(path, r\"1.6_v2\")"
   ],
   "id": "1b13874ae4827107",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "YI, tfas_predict_mat, tfas_actually_mat, mean_error_mat,random_x = SLM_tools.model_training_with_cv(a_reduced,n_components=3,cv_num=10)",
   "id": "cca3f3fdb52ac460",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "SLM_tools.draw_stochastic_landscape_2d(a_reduced,save_path,3)",
   "id": "e49812d6d3304648",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cv_num = 10\n",
    "mean_vec, std_vec, hist_space, x_hist_space, x_ticks, y_ticks = SLM_tools.model_eval(tfas_predict_mat, tfas_actually_mat, cv_num, save_path)"
   ],
   "id": "1f573e1bcbaf88e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tfas_predict_mat_2, tfas_actually_mat_2, mean_error_mat_2 = SLM_tools.train_again_on_validation_and_test(a_reduced, n_components=3)",
   "id": "b0fcdfb79b6312ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "SLM_tools.cv_bias_correction(tfas_predict_mat_2=tfas_predict_mat_2, tfas_actually_mat_2=tfas_actually_mat_2, hist_space=hist_space, mean_vec=mean_vec, x_hist_space=x_hist_space,x_ticks=x_ticks,y_ticks=y_ticks,save_path=save_path)",
   "id": "803562acd8b4be2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b15edd07d5ea6013",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
