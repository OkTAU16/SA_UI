{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%reset -f",
   "id": "33c2da8da1fa6033",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import scipy.io\n",
    "import Rbeast as rb\n",
    "from SLM_tools import *\n",
    "from scipy.io import savemat\n",
    "import pickle\n",
    "import re\n",
    "import time"
   ],
   "id": "b288aa08260ce060",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "top_dir = r\"C:\\Users\\User\\OneDrive - mail.tau.ac.il\\Documents\\Final Project\\Project2-Omri and Idan\\Results\" \n",
    "\n",
    "pattern = re.compile(\n",
    "    r\"(?P<type>distance|energy)_vec_mu_(?P<mu>[-+]?\\d*\\.\\d+|\\d+)_energy_(?P<energy>[-+]?\\d*\\.\\d+|\\d+)_run_num_(?P<run_num>\\d+)_total_num_target_\\d+\\.mat\"\n",
    ")\n",
    "\n",
    "distance_paths = []\n",
    "energy_paths = []\n",
    "seen_distance_paths = set()\n",
    "seen_energy_paths = set()\n",
    "mu = 1.6\n",
    "for root, dirs, files in os.walk(top_dir):\n",
    "    for file in files:\n",
    "        match = pattern.match(file)\n",
    "        if match:\n",
    "            file_info = match.groupdict()\n",
    "            if file_info['mu'] == '1.6':\n",
    "                full_path = os.path.join(root, file)\n",
    "                if file_info['type'] == 'distance':\n",
    "                    if full_path not in seen_distance_paths:\n",
    "                        distance_paths.append(full_path)\n",
    "                        seen_distance_paths.add(full_path)\n",
    "                elif file_info['type'] == 'energy':\n",
    "                    if full_path not in seen_energy_paths:\n",
    "                        energy_paths.append(full_path)\n",
    "                        seen_energy_paths.add(full_path)\n"
   ],
   "id": "1e8eb91d074f320",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(energy_paths)",
   "id": "1d0b082268db2684",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(distance_paths)",
   "id": "2304e90cc1a87716",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "C = []\n",
    "downsampling_factor = 1000\n",
    "n_components = 3\n",
    "features_extracted = 0\n",
    "total_runtime = 0\n",
    "files_with_assembly = 0\n",
    "total_data_files = len(distance_paths)\n",
    "log = []\n",
    "mu = 1.6\n",
    "counters = {\"load\":[],\"extraction\":[],\"segmentation\":[],\"pca\":None,\"extracted_features\":[]}\n",
    "for i, (energy_path, distance_path) in enumerate(zip(energy_paths, distance_paths)):\n",
    "    start1 = time.time()\n",
    "    distance_data = scipy.io.loadmat(distance_path)['foo']\n",
    "    energy_data = scipy.io.loadmat(energy_path)['foo'].T\n",
    "    energy_data, distance_data, time_vec = SLM_tools.downsample(energy_data, distance_data,downsampling_factor)\n",
    "    loading_time = round(time.time() - start1,3)\n",
    "    log1 = f\"\\nfile {i+1}/{len(energy_paths)}  loaded: {os.path.relpath(energy_path,top_dir)} Loading time: {loading_time} seconds\"\n",
    "    print(log1)\n",
    "    log.append(log1)\n",
    "    counters[\"load\"].append(loading_time)\n",
    "    start2 = time.time()\n",
    "    o, cp, mean_trend = SLM_tools.beast(energy_data)\n",
    "    segmentation_time = round(time.time() - start2,3)\n",
    "    log2 = f\"\\nfinished segmentation(Beast), runtime: {segmentation_time} seconds\"\n",
    "    print(log2)\n",
    "    log.append(log2)\n",
    "    counters[\"segmentation\"].append(segmentation_time)\n",
    "    start3 = time.time()\n",
    "    A = SLM_tools.segment_data(energy_data, distance_data,mean_trend,cp)\n",
    "    extraction_time = round(time.time() - start3,3)\n",
    "    print(f\"\\nfinished feature extraction, runtime: {extraction_time} seconds\")\n",
    "    counters[\"extraction\"].append(extraction_time)\n",
    "    print(f\"\\nAssembly? : {len(A) > 0}\")\n",
    "    if len(A) > 0:\n",
    "        files_with_assembly += 1\n",
    "        features_extracted += A[0].shape[0]\n",
    "        counters[\"extracted_features\"].append(A[0].shape[0])\n",
    "        log3 = f\"\\nfiles_with_assembly: {files_with_assembly}/{total_data_files}\"\n",
    "        print(log3)\n",
    "        log.append(log3)\n",
    "        log4 = f\"\\nTotal features_extracted: {features_extracted}\"\n",
    "        print(log4)\n",
    "        log.append(log4)\n",
    "    else:\n",
    "        log5 = f\"\\nNo Assembly, No features extracted\"\n",
    "        print(log5)\n",
    "        log.append(log5)\n",
    "    C.append(A)\n",
    "    total_runtime += (loading_time + extraction_time + segmentation_time)\n",
    "log6 = f\"\\nFinished pre-pca processing,total runtime {total_runtime} for {len(energy_paths)} files\"\n",
    "print(log6)\n",
    "log.append(log6)\n",
    "with open(f\"C_all_mu_{mu}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(C, f)\n",
    "log7 = \"\\nsaved all processed data\"\n",
    "print(log7)\n",
    "log.append(log7)\n",
    "start_pca = time.time()\n",
    "log8 = f\"\\nStarting PCA\"\n",
    "print(log8)\n",
    "log.append(log8)\n",
    "c_reduced = SLM_tools.post_beast_processing(C)\n",
    "principal_components, score, latent = SLM_tools.pca(c_reduced,n_components)\n",
    "a_reduced = SLM_tools.post_pca_processing(score, c_reduced,n_components)\n",
    "runtime_pca = round(time.time() - start_pca,3)\n",
    "counters[\"pca\"] = runtime_pca\n",
    "total_runtime += runtime_pca\n",
    "log9 = f\"\\nFinished PCA runtime {runtime_pca} seconds\"\n",
    "print(log9)\n",
    "log.append(log9)\n",
    "savemat(f\"a_reduced_all_mu_{mu}.mat\", {'a_reduced': a_reduced})\n",
    "log10 = f\"\\nsaved all extracted features , total extracted features {features_extracted} total runtime {total_runtime} seconds\"\n",
    "log.append(log10)\n",
    "log.append(\"SUMMARY\")\n",
    "log11 = f\"\\nfiles {total_data_files} \\nfiles_with_assembly: {files_with_assembly}/{total_data_files} \\nmean extracted features {np.mean(counters['extracted_features'])} \\ntotal extracted features {features_extracted} \\ntotal runtime {total_runtime} seconds \\nmean loading time {np.mean(counters['load'])} \\nmean segmentation time {np.mean(counters['segmentation'])} \\nmean extraction time {np.mean(counters['extraction'])} \\nPCA time {counters['pca']}\"\n",
    "log.append(log11)\n",
    "print(log11)\n",
    "with open(f\"run_log_27_7_mu_{mu}.txt\",'w') as f:\n",
    "    for line in log:\n",
    "        f.write(line)"
   ],
   "id": "cca3f3fdb52ac460",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import scipy.io\n",
    "import Rbeast as rb\n",
    "from SLM_tools import *\n",
    "from scipy.io import savemat\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "%matplotlib inline"
   ],
   "id": "71a17621134fc1a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a_reduced = sio.loadmat(r\"C:\\Users\\User\\OneDrive - mail.tau.ac.il\\Documents\\SA_UI\\not_used\\a_reduced_all_mu_0.6_2.mat\")[\"a_reduced\"]",
   "id": "84c65809e91ac80d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a_reduced.shape",
   "id": "3e6c74da032e5978",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "path=r\"C:\\Users\\User\\OneDrive - mail.tau.ac.il\\Documents\\SA_UI\\not_used\"\n",
    "cv_num = 10"
   ],
   "id": "12d090cd6e8a7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "YI, tfas_predict_mat, tfas_actually_mat, mean_error_mat, train_index, random_x, validation_index, median_fit_vec = SLM_tools.model_training_with_cv(a_reduced,n_components=3,cv_num=10)",
   "id": "9df0c4e221148917",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "mean_vec, std_vec, hist_space, x_hist_space, fig_1, a, b, x_ticks, y_ticks, bin_width= SLM_tools.model_eval(tfas_predict_mat, tfas_actually_mat, cv_num, path)",
   "id": "1f573e1bcbaf88e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tfas_predict_mat_2, tfas_actually_mat_2, mean_error_mat_2 = SLM_tools.train_again_on_validation_and_test(random_x, validation_index, n_components=3)",
   "id": "b0fcdfb79b6312ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "SLM_tools.cv_bias_correction(tfas_predict_mat_2, tfas_actually_mat_2, hist_space,bin_width, mean_vec, x_hist_space,\n",
    "                           median_fit_vec, x_ticks, y_ticks, path)"
   ],
   "id": "778a0318f094d7c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "707%10",
   "id": "e800d02e952e5da2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a38e6fc6acfa0447",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
